---
title: "Classifying New Particle Formation"
subtitle: "Group 128, Viktoria Lilitskaia"
output:
  pdf_document:
    latex_engine: xelatex
date: "2025-12-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

Check the [***Github Repo***](https://github.com/HeljaeRaeisaenen/IML_project) for the source code and additional plots.

Kaggle submissions were made using the same name as stated in the document subtitle (Viktoria Lilitskaia).

Special information about the project: we started as a team of three people, but, unfortunately, my two teammates had to leave the course. Initially, we agreed on using Python and its libraries such as numpy, pandas, etc. to do the project, but now I am switching to R, because I am more familiar with it in data science context. However, some preliminary work such as exploratory data analysis was done using Python in Jupyter Notebook, and this is why the Github repository contains both `.Rmd` and `.ipynb` project files. Now, the primary programming language is R and `project.Rmd` is the main project source code file.

## Custom functions

Under this section, I will define custom functions that I used in this project and provide their code snippets.

### Perplexity

```{r, echo=TRUE}
perplexity <- function(p, true) {
  exp(-mean(log(p)))
}
```

### Alpha and lambda selection for binary glmnet

This function will be used to automatically perform selection of optimal value of $\alpha \in$ {0, 0.5, 1} and $\lambda$ based on the smallest MSE from 10-cross-fold validation using `cv.glmnet`. This is used in selection of models for performing multiclass logistic regression.

```{r, echo=TRUE}
alpha.lambda <- function(data.x, y) {
  X <- model.matrix(~ ., data = data.x)[, -1]

  foldid <- sample(1:10, size=length(y), replace=TRUE)
  alphas <- c(1, 0.5, 0)
  results <- data.frame(alpha = alphas, lambda.min = NA, cvm.min = NA)
  
  for (i in seq_along(alphas)) {
    cv <- cv.glmnet(X, y, alpha=alphas[i], nfolds=10, foldid = foldid)
    results$lambda.min[i] <- cv$lambda.min
    results$cvm.min[i] <- min(cv$cvm)
  }

  best <- results[which.min(results$cvm.min), ]
  
  list(alpha = best$alpha, lambda = best$lambda.min)
}
```

### Binary logistic accuracy and perplexity estimation

This function is used to assess the accuracy and perplexity of a logistic classification method in a binary setting.

```{r, echo=TRUE}
logistic.acc.perp <- function(data, J, y, lambda) {
  n <- floor(0.5*nrow(data))
  acc.binary.lr <- numeric(J)
  perp.lr <- numeric(J)
  
  for (i in 1:J) {
    picked <- sample(seq_len(nrow(data)), size = n)
  
    train <- data[picked, vars]
    valid <- data[-picked, vars]
    y.valid <- y[-picked]
    
    X.train <- model.matrix(~ ., data = train)[, -1]
    y.train <- y[picked]
  
    model <- glmnet(X.train, y.train, family="binomial", alpha=1, lambda=lambda)
  
    valid.matrix <- model.matrix(~ ., data=valid)[, -1]
    valid.pred <- predict(model, newx=valid.matrix, type="response")
    pred <- ifelse(valid.pred >= 0.5, 1, 0)
    
    acc.binary.lr[i] <- mean(pred == y.valid)
    perp.lr[i] <- perplexity(valid.pred)
  }

  list(acc.binary.lr, perp.lr)
}
```

### Multiclass logistic

This function performs multiclass logistic regression using given parameters for a `glmnet` model and returns the most probable class for the test data as well as the full matrix of class probabilities.

```{r, echo=TRUE}
logistic.multiclass <- function(train.x, train.y, test, alphas, lambdas, classes) {
  results <- matrix(data=NA, nrow=nrow(test), ncol=length(classes))
  
  X <- model.matrix(~ ., data = train.x)[, -1]
  test.matrix <- model.matrix(~ ., data=test)[, -1]

  for (i in seq_along(classes)) {
    fit <- glmnet(X, train.y[, i], family="binomial", alpha=alphas[i], lambda=lambdas[i])
    pred <- predict(fit, newx = test.matrix, type="response")
    results[, i] <- pred
  }
  
  # Index of max probability per row
  max_id <- max.col(results, ties.method = "first")
  # Assign class
  predicted <- classes[max_id]
  return(list(prob = results, predicted = predicted))
}
```

# Preprocessing

As a part of preprocessing the data, we need to remove unnecessary columns such as `date`, `id`, and `partlybad`. We also need to encode our class values as numeric values. First, when dealing with binary classification, we encode *nonevent* as 0 and *any event* as 1.

In a multiclass setting, preprocessing of response values can vary depending on the method used. For example, multinomial regression that we use later requires classes to be parsed through a `factor` function to make them categorical. I assume this method to be a default in the most multiclass classification methods in R.

Later, in preprocessing for multiclass logistic regression for "one-vs-all" one class out of four was encoded as 1 and the rest as 0.

```{r}
test <- read.csv("test.csv")

npf <- read.csv("train.csv")
npf <- subset(npf, select=-c(date, id, partlybad))

test.ids <- test$id
test <- subset(test, select=-c(date, id, partlybad))

vars <- colnames(npf)
vars <- vars[! vars %in% c('class4')]

y.binary <- ifelse(npf$class4 == "nonevent", 0, 1)
y.multi <- factor(npf$class4)
```

## Normalization

We can also normalize our data, because generally normalizing yields more stable results in some classification algorithms such as SVMs or logistic regression. This however can depend on an algorithm, but we can try classification both with and without normalization and compare the results.

```{r}
npf.scaled <- as.data.frame(scale(npf[, vars]))
```

# Data Exploration

This task was done using Python and the Seaborn library.

My main task in data exploration was to produce pairplots of variables in the data set to study possible collinearity issues and get more understanding of data overall. However, there is a problem in studying pairplots: we have 100 variables! To deal with this problem, I decided to plot pairplots of 5 variables at a time. Unfortunately, this reduced the number of predictors we can view at the same time, but was useful in the sense of readability. As a result, I produced in total of 20 pairplot images which are saved as `.png` files in the `plots` folder in the Github repository.

I also did a mistake in code once, which produced a pairplot image of 20 variables at the same time. It was unintended, but I saved the image in the `plots` folder anyway, because it gave some broader insight into the variables.

![An example of a pairplot produced by Seaborn. You can find more examples in the "plots" folder.](./plots/pairplot_1.png)

As we see from the generated images, there is a large amount of features that are highly linearly correlated, some of them producing strictly linear plots. We will have to address this issue in the feature selection process.

## Pearson Correlation Coefficient

We can construct a correlation matrix between all variables to further detect correlations.

```{r}
corr <- cor(npf[, vars])
thr <- 0.7

corr_df <- which(abs(corr) > thr, arr.ind = TRUE)
# Remove diagonal and duplicates
corr_df <- corr_df[corr_df[, 1] < corr_df[, 2], ]

result <- data.frame(
  var1 = rownames(corr)[corr_df[, 1]],
  var2 = colnames(corr)[corr_df[, 2]],
  corr = corr[corr_df]
)

table(result$var2)
```

In this table, we see how many times a variable appears in a pair of highly correlated predictors whose Pearson correlation is \> 0.7. There are too many correlated pairs to print them all at once, so we decided only to print their counts.

## Feature selection

As noticed before, the data has significant amount of collinear variables, making a lot of features useless for classification. We can use different methods to detect problems further. For the time being, we decided to use feature selection method already embedded in model training algorithm later, such as Lasso/Ridge implemented in `glmnet` library to be used in logistic regression scenario. We will assess the most suitable feature shrinkage method by comparing different values of $\alpha$ used in `glmnet`.

# Binary classification: logistic regression

In this section, we will compare different models for binary classification using logistic regression and comparing different settings. Logistic regression was chosen because it is a solid baseline classification approach that is easy to implement and analyze, as well as having built-in methods for variable selection such as in `glmnet`.

## Setup

Because standard `glm` has convergence issues on the full dataset, I will be using `glmnet` library and experiment with different values of $\alpha$ to see what gives the best accuracy on the validation set. I will be using k-fold cross validation to measure each model's squared loss and mean absolute error to choose the best-performing model. I will choose `k=10` and CV implementation by the R library `glmnet`. I will try values of $\alpha$ = 0, 0.5 and 1.

I will use both scaled and unscaled data sets to see which one gives better performance.

```{r}
library(glmnet)
```

```{r}
X <- model.matrix(~ ., data = npf.scaled)[, -1]
y <- y.binary

X.u <- model.matrix(~ ., data = npf[, vars])[, -1]

foldid <- sample(1:10, size=length(y), replace=TRUE)

cv1 <- cv.glmnet(X, y, alpha=1, nfolds=10, type.measure = "mse", foldid = foldid)
cv.5 <- cv.glmnet(X, y, alpha=0.5, nfolds=10, type.measure = "mse", foldid = foldid)
cv0 <- cv.glmnet(X, y, alpha=0, nfolds=10, type.measure = "mse", foldid = foldid)

cv1.u <- cv.glmnet(X.u, y, alpha=1, nfolds=10, type.measure = "mse", foldid = foldid)
cv.5.u <- cv.glmnet(X.u, y, alpha=0.5, nfolds=10, type.measure = "mse", foldid = foldid)
cv0.u <- cv.glmnet(X.u, y, alpha=0, nfolds=10, type.measure = "mse", foldid = foldid)
```

We can plot MSE based on values of $\log \lambda$ and choose the best value of $\alpha$.

```{r}
plot(log(cv1$lambda), cv1$cvm, pch=19, col="red", xlab="log(lambda)", ylab="MSE", main="")
points(log(cv.5$lambda), cv.5$cvm, pch=19, col="purple")
points(log(cv0$lambda), cv0$cvm, pch=19, col="blue")
legend(
  x = "topleft",
  legend = c("alpha=1", "alpha=0.5", "alpha=0"),
  pch = 19,
  col = c("red", "purple", "blue")
)
```

According to the plot, the absolute lowest MSE is reached by $\alpha=1$. We can also compare the lowest values of MSE by code:

```{r}
print("Lowest MSE when alpha=1:")
cv1$cvm[cv1$lambda == cv1$lambda.min]
print("Lowest MSE when alpha=0.5:")
cv.5$cvm[cv.5$lambda == cv.5$lambda.min]
print("Lowest MSE when alpha=0:")
cv0$cvm[cv0$lambda == cv0$lambda.min]
```

So, to fit our model, we choose $\alpha=1$ and the lambda that gives the minimum MSE in `cv1`.

## Accuracy and perplexity

We can estimate the accuracy of our classification by choosing a number of random validation sets and training the model with selected parameters on the rest of the training set. I will run the code to select a random training (50%) and validation (50%) sets and estimate the accuracy as well the perplexity, using our custom defined `perplexity` function.

```{r}
accperp.s <- logistic.acc.perp(npf.scaled, 100, y.binary, cv1$lambda.min)
acc.s <- accperp.s[[1]]
perp.s <- accperp.s[[2]]

accperp.u <- logistic.acc.perp(npf, 100, y.binary, cv1.u$lambda.min)
acc.u <- accperp.u[[1]]
perp.u <- accperp.u[[2]]
```

```{r}
hist1.s <- hist(acc.s, plot=FALSE)
hist2.s <- hist(perp.s, plot=FALSE)

hist1.u <- hist(acc.u, plot=FALSE)
hist2.u <- hist(perp.u, plot=FALSE)

par(mfrow=c(1, 2))

plot(hist1.s, col = rgb(1,0,0,0.4), xlab="Accuracy", main="Accuracy", ylim = c(0, 40))
plot(hist1.u, xaxt='n', yaxt = 'n', col = rgb(0,0,1,0.4), add=TRUE)

legend(
  x = "topright",
  legend = c("Scaled", "Unscaled"),
  col = c("red", "blue"),
  lwd = 2
)

plot(hist2.s, col = rgb(1,0,0,0.4), xlab="Perplexity", main="Perplexity", ylim = c(0, 40))
plot(hist2.u, xaxt='n', yaxt = 'n', col = rgb(0,0,1,0.4), add=TRUE)

legend(
  x = "topright",
  legend = c("Scaled", "Unscaled"),
  col = c("red", "blue"),
  lwd = 2
)
```
Mean and standard deviation values of accuracies between scaled and unscaled data:

```{r}
print("Mean accuracy and std for scaled data:")
mean(acc.s)
sd(acc.s)
print("Mean accuracy and std for unscaled data:")
mean(acc.u)
sd(acc.u)
```

Based on the mean and standard deviation values and histogram comparison, we do not notice any considerable differences between scaling or not scaling the data.

# Multiclass classification

There are different ways to approach multiclass classification. One can utilize a ready-to-run multinomial algorithm such as multinomial family in the `glmnet` library that can deal with multiple classes. We will test this option first, because it is the easiest to setup.

## Multinomial regression

Additionally to the parameter $\alpha$, the multinomial regression has a parameter $q \in$ {1, 2}. $q=1$ means lasso penalties for each parameter and $q=2$ means a grouped lasso penalty on all the coefficients of a variable. We will try both values and assess which one works better for our data set using `cv.glmnet` as in the binary classification task with K = 10, as well as testing different values of $\alpha \in$ {1, 0.5, 0}. We will use the scaled data set.

After running the code for both values of q and all three values of alpha, I've had convergence issues with $\alpha = 1, q = 1$, $\alpha = 1, q = 2$, $\alpha = 0.5, q = 2$. So, we will remove these models from the test and compare the rest which did not have convergence issues.

```{r}
X <- model.matrix(~ ., data = npf.scaled)[, -1]
y <- y.multi

foldid <- sample(1:10, size=length(y), replace=TRUE)

cv.5q1 <- cv.glmnet(X, y, alpha=0.5, nfolds=10, type.measure = "mse", foldid = foldid, family="multinomial", type.multinomial="ungrouped")
cv0q1 <- cv.glmnet(X, y, alpha=0, nfolds=10, type.measure = "mse", foldid = foldid, family="multinomial", type.multinomial="ungrouped")

cv0q2 <- cv.glmnet(X, y, alpha=0, nfolds=10, type.measure = "mse", foldid = foldid, family="multinomial", type.multinomial="grouped")
```

Now, we can plot MSE based on values of $\log \lambda$ and choose the best value of $\alpha$ and $q$.

```{r}
plot(log(cv.5q1$lambda), cv.5q1$cvm, pch=19, col="red", xlab="log(lambda)", ylab="MSE")
points(log(cv0q1$lambda), cv0q1$cvm, pch=19, col="purple")
points(log(cv0q2$lambda), cv0q2$cvm, pch=19, col="blue")
legend(
  x = "topleft",
  legend = c("alpha=0.5, q=1", "alpha=0, q=1", "alpha=0, q=2"),
  pch = 19,
  col = c("red", "purple", "blue")
)
```

```{r}
print("Lowest MSE when alpha=0.5, q=1:")
cv.5q1$cvm[cv.5q1$lambda == cv.5q1$lambda.min]
print("Lowest MSE when alpha=0, q=1:")
cv0q1$cvm[cv0q1$lambda == cv0q1$lambda.min]
print("Lowest MSE when alpha=0, q=2:")
cv0q2$cvm[cv0q2$lambda == cv0q2$lambda.min]
```

Based on these criteria, we will choose parameters $\alpha=0.5, q=1$.

Estimating accuracy:

```{r}
n <- floor(0.5*nrow(npf.scaled))
J <- 30
acc.multi1 <- numeric(J)

for (i in 1:J) {
  picked <- sample(seq_len(nrow(npf.scaled)), size = n)

  train <- npf.scaled[picked, ]
  valid <- npf.scaled[-picked, ]
  y.valid <- y.multi[-picked]
  
  X.train <- model.matrix(~ ., data = train)[, -1]
  y.train <- y.multi[picked]

  model <- glmnet(X.train, y.train, family="multinomial", alpha=0.5, lambda=cv.5q1$lambda.min, type.multinomial="ungrouped")

  valid.matrix <- model.matrix(~ ., data=valid)[, -1]
  valid.pred <- predict(model, newx=valid.matrix, type="class")
  
  acc.multi1[i] <- mean(valid.pred == y.valid)
}
```

```{r}
hist(acc.multi1, xlab="Accuracy", main="Expected accuracy from multinomial classification")
```

We can expect our multinomial classification accuracy to be on average between 0.62 and 0.68.

Then, we can fit the model on the full data set:

```{r}
model.multi <- glmnet(X, y, family="multinomial", alpha=0.5, lambda=cv.5q1$lambda.min, type.multinomial="ungrouped")
print(model.multi)
```

## Logistic regression

Another possible solution is to train several models using binary logistic regression and make them "fight" against each other either in one-vs-one or one-vs-all settings.

We can setup our logistic regression to give predictions in a multiclass setting. We can use either "one-vs-all" or "one-vs-one" strategy. For this problem, we chose "one-vs-all", because it requires less amount of models to train, making it more simple option. We will test this approach on scaled and unscaled data.

In this strategy, we will train four separate logistic regression models using `glmnet`: *nonevent vs. rest*, *Ia vs. rest*, *Ib vs. rest*, *II vs. rest*. We will use the same strategy as in binary classification problem to choose the appropriate value of $\alpha$ for each model. After that, we will run our test data set through all four models and assign class with the largest probability.

### Nonevent vs. rest

This one is similar to the binary classification of nonevent vs. event, but in this case our model will give the probability of *nonevent* instead of probability of *event*.

```{r}
y.m.non.rest <- ifelse(npf$class4 == "nonevent", 1, 0)
al1 <- alpha.lambda(npf.scaled, y.m.non.rest)
alpha1 <- al1[[1]]
lambda1 <- al1[[2]]
print(paste0("Nonevent vs. rest: alpha=", alpha1, " lambda=", lambda1))
```

### Ia vs. rest

```{r}
y.m.ia.rest <- ifelse(npf$class4 == "Ia", 1, 0)
al2 <- alpha.lambda(npf.scaled, y.m.ia.rest)
alpha2 <- al2[[1]]
lambda2 <- al2[[2]]
print(paste0("Ia vs. rest: alpha=", alpha2, " lambda=", lambda2))
```

### Ib vs. rest

```{r}
y.m.ib.rest <- ifelse(npf$class4 == "Ib", 1, 0)
al3 <- alpha.lambda(npf.scaled, y.m.ib.rest)
alpha3 <- al3[[1]]
lambda3 <- al3[[2]]
print(paste0("Ib vs. rest: alpha=", alpha3, " lambda=", lambda3))
```

### II vs. rest

```{r}
y.m.ii.rest <- ifelse(npf$class4 == "II", 1, 0)
al4 <- alpha.lambda(npf.scaled, y.m.ii.rest)
alpha4 <- al4[[1]]
lambda4 <- al4[[2]]
print(paste0("II vs. rest: alpha=", alpha4, " lambda=", lambda4))
```

### Accuracy estimation

```{r}
alphas <- c(alpha1, alpha2, alpha3, alpha4)
lambdas <- c(lambda1, lambda2, lambda3, lambda4)
classes <- c("nonevent", "Ia", "Ib", "II")
```

```{r}
n <- floor(0.5*nrow(npf.scaled))
J <- 70
acc.multi2 <- numeric(J)

for (i in 1:J) {
  picked <- sample(seq_len(nrow(npf.scaled)), size = n)

  train <- npf.scaled[picked, ]
  valid <- npf.scaled[-picked, ]

  y.train1 <- y.m.non.rest[picked]
  y.train2 <- y.m.ia.rest[picked]
  y.train3 <- y.m.ib.rest[picked]
  y.train4 <- y.m.ii.rest[picked]
  y <- cbind(y.train1, y.train2, y.train3, y.train4)
  
  y.valid <- npf$class4[-picked]

  res <- logistic.multiclass(train, y, valid, alphas, lambdas, classes)
  
  acc.multi2[i] <- mean(y.valid == res$predicted)
}
```

```{r}
hist(acc.multi2, xlab="Accuracy", main="Expected accuracy from one-vs-all logistic regression (scaled)")
```

Expected accuracy is comparable to multinomial regression. Next, we will try follow the same steps with unscaled data.

Accuracy estimation (unscaled)

```{r}
al1 <- alpha.lambda(npf[, vars], y.m.non.rest)
alpha1 <- al1[[1]]
lambda1 <- al1[[2]]
print(paste0("Nonevent vs. rest: alpha=", alpha1, " lambda=", lambda1))
```

-   *Ia vs. rest*

```{r}
al2 <- alpha.lambda(npf[, vars], y.m.ia.rest)
alpha2 <- al2[[1]]
lambda2 <- al2[[2]]
print(paste0("Ia vs. rest: alpha=", alpha2, " lambda=", lambda2))
```

-   *Ib vs. rest*

```{r}
al3 <- alpha.lambda(npf[, vars], y.m.ib.rest)
alpha3 <- al3[[1]]
lambda3 <- al3[[2]]
print(paste0("Ib vs. rest: alpha=", alpha3, " lambda=", lambda3))
```

-   *II vs. rest*

```{r}
al4 <- alpha.lambda(npf[, vars], y.m.ii.rest)
alpha4 <- al4[[1]]
lambda4 <- al4[[2]]
print(paste0("II vs. rest: alpha=", alpha4, " lambda=", lambda4))
```

*Accuracy estimation*

```{r}
alphas2 <- c(alpha1, alpha2, alpha3, alpha4)
lambdas2 <- c(lambda1, lambda2, lambda3, lambda4)
```

```{r}
n <- floor(0.5*nrow(npf))
J <- 70
acc.multi3 <- numeric(J)

for (i in 1:J) {
  picked <- sample(seq_len(nrow(npf)), size = n)

  train <- npf[picked, vars]
  valid <- npf[-picked, vars]

  y.train1 <- y.m.non.rest[picked]
  y.train2 <- y.m.ia.rest[picked]
  y.train3 <- y.m.ib.rest[picked]
  y.train4 <- y.m.ii.rest[picked]
  y <- cbind(y.train1, y.train2, y.train3, y.train4)
  
  y.valid <- npf$class4[-picked]

  res <- logistic.multiclass(train, y, valid, alphas2, lambdas2, classes)
  
  acc.multi3[i] <- mean(y.valid == res$predicted)
}
```

```{r}
hist(acc.multi3, xlab="Accuracy", main="Expected accuracy from one-vs-all logistic regression (unscaled)")
```

We notice that on average, this one-vs-all classifier performs better on unscaled data.

# Results

```{r}
rows <- c("Scaled Logistic Regression (LR)", "Scaled LR (one-vs-all)", "Unscaled LR (one-vs-all)", "Multinomial Regression")

accs.b <- c(mean(acc.binary.lr), NA, NA, NA)
accs.m <- c(NA, mean(acc.multi2), mean(acc.multi3), mean(acc.multi1))
perps <- c(mean(perp.lr), NA, NA, NA)
kaggle <- c(0.74645, NA, 0.76250, 0.75559)

table.df <- data.frame(
  Model = rows,
  'Bin.accuracy' = accs.b,
  'Mult.accuracy' = accs.m,
  Perplexity = perps,
  'Kaggle.score' = kaggle,
  stringsAsFactors = FALSE
)

knitr::kable(table.df)
```

After trying both scaled and unscaled data on Logistic Regression and Multinomial Regression, I notices with surprise that unscaled data produced better results on average. My first two submissions I used unscaled data on both binary and multiclass classification, which gave much better Kaggle score than the third submission with scaled data. Also, the average perplexity was estimated lower in unscaled data cases than in scaled. In case of unscaled data for multiclass problem, one-vs-all Logistic Regression did better then Multinomial. With scaled data, however, Multinomial Regression managed to get slightly more accuracy on average.